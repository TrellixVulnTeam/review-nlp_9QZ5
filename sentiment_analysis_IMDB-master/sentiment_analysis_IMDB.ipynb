{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Sentiment Analysis IMDB\n",
    "\n",
    "This notebook is a simple straight-forward way to achieve 90% accuracy on IMDB dataset. Note that this is not the only way to achieve such accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "imdb loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9675</td>\n",
       "      <td>I admit to being somewhat jaded about the movie genre of a young child softening the heart of his/her reluctant guardian. I've seen enough of them  Baby Boom, Kolya, About a Boy, Mostly Martha, and to some extent, Whale Rider  to expect to be bored by the formula. What held my attention in The King of Masks was the grimness of the setting: small-town China in the 1930's. Extreme poverty was the norm, and girl children were considered so worthless to poor parents that they killed them at bi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20233</td>\n",
       "      <td>The person who wrote the glowing review of this misguided project must be related to the writer/director/star--or is, in fact, the same person as it defies rational thinking that this movie would be appealing to anyone not connected to a very tightly woven inner circle. How about this? You want to make a movie--tell a story; entertain; draw me in with vivid characters. Sure, you can do it artfully without bowing to the commercial elements designed for mass appeal. However, do not address ele...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7609</td>\n",
       "      <td>A prison cell.Four prisoners-Carrere,a young company director accused of fraud,35 year old transsexual in the process of his transformation, Daisy,a 20 year-old mentally challenged idiot savant and Lassalle,a 60 year-old intellectual who murdered his wife.Behind a stone slab in the cell,mysteriously pulled loose,they discovered a book:the diary of a former prisoner,Danvers,who occupied the cell at the beginning of the century.The diary contains magic formulas that supposedly enable prisoners...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "9675   I admit to being somewhat jaded about the movie genre of a young child softening the heart of his/her reluctant guardian. I've seen enough of them  Baby Boom, Kolya, About a Boy, Mostly Martha, and to some extent, Whale Rider  to expect to be bored by the formula. What held my attention in The King of Masks was the grimness of the setting: small-town China in the 1930's. Extreme poverty was the norm, and girl children were considered so worthless to poor parents that they killed them at bi...   \n",
       "20233  The person who wrote the glowing review of this misguided project must be related to the writer/director/star--or is, in fact, the same person as it defies rational thinking that this movie would be appealing to anyone not connected to a very tightly woven inner circle. How about this? You want to make a movie--tell a story; entertain; draw me in with vivid characters. Sure, you can do it artfully without bowing to the commercial elements designed for mass appeal. However, do not address ele...   \n",
       "7609   A prison cell.Four prisoners-Carrere,a young company director accused of fraud,35 year old transsexual in the process of his transformation, Daisy,a 20 year-old mentally challenged idiot savant and Lassalle,a 60 year-old intellectual who murdered his wife.Behind a stone slab in the cell,mysteriously pulled loose,they discovered a book:the diary of a former prisoner,Danvers,who occupied the cell at the beginning of the century.The diary contains magic formulas that supposedly enable prisoners...   \n",
       "\n",
       "      sentiment  \n",
       "9675        pos  \n",
       "20233       neg  \n",
       "7609        pos  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nlp_proj_utils as utils\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth', 500)  # Set display column width to show more content\n",
    "\n",
    "# Load dataset, download if necessary\n",
    "train, test = utils.get_imdb_dataset()\n",
    "\n",
    "# Get a sample (head) of the data frame\n",
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "\n",
    "In this part,I will remove all the html label,punctuation and stopwords from the dataset. In order to reach a higher accuracy, I have selected 3000 most common word in the training data, and only the word in this list will be kept for further anylysis.\n",
    "1. Remove HTML tag (<br /> in this case) from the review text\n",
    "2. Remove punctuations (replace with whitespace)\n",
    "3. Split review text into tokens\n",
    "4. Remove tokens that are considered as \"stopwords\"\n",
    "5. For the rest, do lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "transtbl = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy several book yesterday really love'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a text input and return the preprocessed string.\n",
    "def preprocessing(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Take a text input and return the preprocessed string.\n",
    "    i.e.: preprocessed tokens concatenated by whitespace\n",
    "    \"\"\"\n",
    "    line = line.replace('<br />','').translate(transtbl)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v')\n",
    "              for t in nltk.word_tokenize(line)\n",
    "              if t.lower() not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessing(\"I bought several books yesterday<br /> and I really love them!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tqdm/_tqdm.py:634: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687a0f86155345b4bf11e01887357faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b23fc970f74b398d2e7a61e6d8d6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "for df in train, test:\n",
    "    df['text_prep'] = df['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14657</td>\n",
       "      <td>Julia Stiles is a talented young actress, who with guidance from a reputable agent has a lot of potential. Obviously, the person who guided her into this travesty is not someone who cares anything about her career. I sat in the theater surrounded by teenagers who left in droves to find another movie to sneak into wondering who thought this movie would appeal to anyone. It was poorly written, the casting director could only have put 1 or 2 minutes of effort into the characters and the directo...</td>\n",
       "      <td>neg</td>\n",
       "      <td>julia stiles talented young actress guidance reputable agent lot potential obviously person guide travesty someone care anything career sit theater surround teenagers leave droves find another movie sneak wonder think movie would appeal anyone poorly write cast director could put 1 2 minutes effort character director obviously care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3926</td>\n",
       "      <td>As a long time fan of Peter O'Donnell's greatest creation, I watched this film on DVD with no great hopes of enjoyment; indeed I expected to be reaching in disgust for the remote control within fifteen minutes. But instead I thoroughly enjoyed this production, and I especially enjoyed and appreciated how the producers and director succeeded in telling the Modesty Blaise back story. They managed to avoid the trap of making a (bad) film version of the books we are all so familiar with, choosin...</td>\n",
       "      <td>pos</td>\n",
       "      <td>long time fan peter donnell greatest creation watch film dvd great hop enjoyment indeed expect reach disgust remote control within fifteen minutes instead thoroughly enjoy production especially enjoy appreciate producers director succeed tell modesty blaise back story manage avoid trap make bad film version book familiar choose instead concentrate period modesty life allude novels production value student cinematography yes film film tight financial time budget maybe show spoil viewer enjoym...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "14657  Julia Stiles is a talented young actress, who with guidance from a reputable agent has a lot of potential. Obviously, the person who guided her into this travesty is not someone who cares anything about her career. I sat in the theater surrounded by teenagers who left in droves to find another movie to sneak into wondering who thought this movie would appeal to anyone. It was poorly written, the casting director could only have put 1 or 2 minutes of effort into the characters and the directo...   \n",
       "3926   As a long time fan of Peter O'Donnell's greatest creation, I watched this film on DVD with no great hopes of enjoyment; indeed I expected to be reaching in disgust for the remote control within fifteen minutes. But instead I thoroughly enjoyed this production, and I especially enjoyed and appreciated how the producers and director succeeded in telling the Modesty Blaise back story. They managed to avoid the trap of making a (bad) film version of the books we are all so familiar with, choosin...   \n",
       "\n",
       "      sentiment  \\\n",
       "14657       neg   \n",
       "3926        pos   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text_prep  \n",
       "14657                                                                                                                                                                        julia stiles talented young actress guidance reputable agent lot potential obviously person guide travesty someone care anything career sit theater surround teenagers leave droves find another movie sneak wonder think movie would appeal anyone poorly write cast director could put 1 2 minutes effort character director obviously care  \n",
       "3926   long time fan peter donnell greatest creation watch film dvd great hop enjoyment indeed expect reach disgust remote control within fifteen minutes instead thoroughly enjoy production especially enjoy appreciate producers director succeed tell modesty blaise back story manage avoid trap make bad film version book familiar choose instead concentrate period modesty life allude novels production value student cinematography yes film film tight financial time budget maybe show spoil viewer enjoym...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0697157a1fc74b4591e6b45ec9d2e310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = [w for text in tqdm_notebook(train['text_prep']) \n",
    "             for w in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 65102 samples and 3025774 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Use FreqDist to get count for each word\n",
    "voca = nltk.FreqDist(all_words)\n",
    "print(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 48184),\n",
       " ('movie', 44024),\n",
       " ('one', 26785),\n",
       " ('make', 23568),\n",
       " ('like', 22361),\n",
       " ('see', 20792),\n",
       " ('get', 18140),\n",
       " ('time', 16167),\n",
       " ('good', 15140),\n",
       " ('character', 14172)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = [word for word, _ in voca.most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "import nlp_proj_utils as utils\n",
    "from tensorflow.keras.models import Model  \n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "loading glove... this may take a while...\n",
      "glove loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "word_to_index, word_to_vec_map = utils.load_glove_vecs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Select the first 200 words for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of words in a sentence: 200\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200\n",
    "print('max number of words in a sentence:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training/testing features into index list\n",
    "train_text = utils.sentences_to_indices(train['text_prep'], word_to_index, maxlen, topwords)\n",
    "test_text = utils.sentences_to_indices(test['text_prep'], word_to_index, maxlen, topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[251034., 160418., 306501., ...,      0.,      0.,      0.],\n",
       "       [ 77324., 181890., 251034., ...,      0.,      0.,      0.],\n",
       "       [336968., 148224., 236880., ...,      0.,      0.,      0.],\n",
       "       ...,\n",
       "       [268508.,  61762., 251057., ...,      0.,      0.,      0.],\n",
       "       [134390.,  44995.,  74804., ...,      0.,      0.,      0.],\n",
       "       [125377., 251057., 303435., ...,      0.,      0.,      0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert label to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "test_y = test['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return a Keras Embedding Layer given word_to_vec mapping and word_to_index mapping\n",
    "    \n",
    "    Args:\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "\n",
    "    Return:\n",
    "        Keras.layers.Embedding: Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keras requires vocab length start from index 1\n",
    "    vocab_len = len(word_to_index) + 1  \n",
    "    emb_dim = list(word_to_vec_map.values())[0].shape[0]\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    return Embedding(\n",
    "        input_dim=vocab_len,\n",
    "        output_dim=emb_dim,\n",
    "        trainable=False,  # Indicating this is a pre-trained embedding \n",
    "        weights=[emb_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a LSTM Model\n",
    "\n",
    "I will use a two layer LSTM Model to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return the Keras model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: The dim of input layer\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Keras.models.Model: 2-layer LSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    sentence_indices = Input(shape=(input_dim,), dtype='int32')\n",
    "    \n",
    "    # Build embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_index, word_to_vec_map)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # 2-layer LSTM\n",
    "    X = LSTM(128, return_sequences=True, recurrent_dropout=0.5)(embeddings)  # N->N RNN，得到所有的a\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = LSTM(128, recurrent_dropout=0.5)(X)  # N -> 1 RNN\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "imdb_model = build_model(\n",
    "    maxlen, \n",
    "    word_to_index, \n",
    "    word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 192, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 192, 128)          91648     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 20,141,171\n",
      "Trainable params: 141,121\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/quantumfram/opt/anaconda3/envs/nlp_proj/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "imdb_model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/200\n",
      "25000/25000 [==============================] - 345s 14ms/sample - loss: 0.6955 - acc: 0.5212 - val_loss: 0.6905 - val_acc: 0.5303\n",
      "Epoch 2/200\n",
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.6882 - acc: 0.5388 - val_loss: 0.7060 - val_acc: 0.5013\n",
      "Epoch 3/200\n",
      "25000/25000 [==============================] - 376s 15ms/sample - loss: 0.6901 - acc: 0.5330 - val_loss: 0.6886 - val_acc: 0.5331\n",
      "Epoch 4/200\n",
      "25000/25000 [==============================] - 408s 16ms/sample - loss: 0.6837 - acc: 0.5540 - val_loss: 0.6768 - val_acc: 0.5545\n",
      "Epoch 5/200\n",
      "25000/25000 [==============================] - 380s 15ms/sample - loss: 0.6766 - acc: 0.5791 - val_loss: 0.6534 - val_acc: 0.6177\n",
      "Epoch 6/200\n",
      "25000/25000 [==============================] - 409s 16ms/sample - loss: 0.6693 - acc: 0.5894 - val_loss: 0.6158 - val_acc: 0.6830\n",
      "Epoch 7/200\n",
      "25000/25000 [==============================] - 396s 16ms/sample - loss: 0.6487 - acc: 0.6370 - val_loss: 0.5860 - val_acc: 0.7059\n",
      "Epoch 8/200\n",
      "25000/25000 [==============================] - 347s 14ms/sample - loss: 0.5696 - acc: 0.7276 - val_loss: 0.5112 - val_acc: 0.7644\n",
      "Epoch 9/200\n",
      "25000/25000 [==============================] - 332s 13ms/sample - loss: 0.5138 - acc: 0.7655 - val_loss: 0.4867 - val_acc: 0.7894\n",
      "Epoch 10/200\n",
      "25000/25000 [==============================] - 329s 13ms/sample - loss: 0.4877 - acc: 0.7800 - val_loss: 0.4584 - val_acc: 0.7972\n",
      "Epoch 11/200\n",
      "25000/25000 [==============================] - 335s 13ms/sample - loss: 0.4752 - acc: 0.7884 - val_loss: 0.4559 - val_acc: 0.7979\n",
      "Epoch 12/200\n",
      "25000/25000 [==============================] - 351s 14ms/sample - loss: 0.4709 - acc: 0.7900 - val_loss: 0.4534 - val_acc: 0.8090\n",
      "Epoch 13/200\n",
      "25000/25000 [==============================] - 342s 14ms/sample - loss: 0.4562 - acc: 0.7958 - val_loss: 0.4363 - val_acc: 0.8132\n",
      "Epoch 14/200\n",
      "25000/25000 [==============================] - 345s 14ms/sample - loss: 0.4471 - acc: 0.8014 - val_loss: 0.4578 - val_acc: 0.8150\n",
      "Epoch 15/200\n",
      "25000/25000 [==============================] - 360s 14ms/sample - loss: 0.4382 - acc: 0.8059 - val_loss: 0.4336 - val_acc: 0.8171\n",
      "Epoch 16/200\n",
      "25000/25000 [==============================] - 350s 14ms/sample - loss: 0.4368 - acc: 0.8102 - val_loss: 0.4370 - val_acc: 0.8232\n",
      "Epoch 17/200\n",
      "25000/25000 [==============================] - 352s 14ms/sample - loss: 0.4265 - acc: 0.8151 - val_loss: 0.4194 - val_acc: 0.8247\n",
      "Epoch 18/200\n",
      "25000/25000 [==============================] - 368s 15ms/sample - loss: 0.4227 - acc: 0.8164 - val_loss: 0.4372 - val_acc: 0.8272\n",
      "Epoch 19/200\n",
      "25000/25000 [==============================] - 356s 14ms/sample - loss: 0.4173 - acc: 0.8194 - val_loss: 0.4194 - val_acc: 0.8256\n",
      "Epoch 20/200\n",
      "25000/25000 [==============================] - 358s 14ms/sample - loss: 0.4135 - acc: 0.8218 - val_loss: 0.4075 - val_acc: 0.8238\n",
      "Epoch 21/200\n",
      "25000/25000 [==============================] - 366s 15ms/sample - loss: 0.4046 - acc: 0.8254 - val_loss: 0.3969 - val_acc: 0.8301\n",
      "Epoch 22/200\n",
      "25000/25000 [==============================] - 358s 14ms/sample - loss: 0.4003 - acc: 0.8280 - val_loss: 0.4054 - val_acc: 0.8284\n",
      "Epoch 23/200\n",
      "25000/25000 [==============================] - 371s 15ms/sample - loss: 0.4076 - acc: 0.8228 - val_loss: 0.3954 - val_acc: 0.8315\n",
      "Epoch 24/200\n",
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.3935 - acc: 0.8344 - val_loss: 0.3974 - val_acc: 0.8364\n",
      "Epoch 25/200\n",
      "25000/25000 [==============================] - 360s 14ms/sample - loss: 0.3895 - acc: 0.8340 - val_loss: 0.3871 - val_acc: 0.8399\n",
      "Epoch 26/200\n",
      "25000/25000 [==============================] - 375s 15ms/sample - loss: 0.3847 - acc: 0.8361 - val_loss: 0.3757 - val_acc: 0.8380\n",
      "Epoch 27/200\n",
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.3815 - acc: 0.8375 - val_loss: 0.3729 - val_acc: 0.8388\n",
      "Epoch 28/200\n",
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.3744 - acc: 0.8415 - val_loss: 0.3742 - val_acc: 0.8429\n",
      "Epoch 29/200\n",
      "25000/25000 [==============================] - 378s 15ms/sample - loss: 0.3730 - acc: 0.8431 - val_loss: 0.3764 - val_acc: 0.8448\n",
      "Epoch 30/200\n",
      "25000/25000 [==============================] - 363s 15ms/sample - loss: 0.3696 - acc: 0.8409 - val_loss: 0.3594 - val_acc: 0.8454\n",
      "Epoch 31/200\n",
      "25000/25000 [==============================] - 363s 15ms/sample - loss: 0.3650 - acc: 0.8469 - val_loss: 0.3689 - val_acc: 0.8458\n",
      "Epoch 32/200\n",
      "25000/25000 [==============================] - 396s 16ms/sample - loss: 0.3615 - acc: 0.8500 - val_loss: 0.3640 - val_acc: 0.8456\n",
      "Epoch 33/200\n",
      "25000/25000 [==============================] - 692s 28ms/sample - loss: 0.3572 - acc: 0.8507 - val_loss: 0.3530 - val_acc: 0.8485\n",
      "Epoch 34/200\n",
      "25000/25000 [==============================] - 521s 21ms/sample - loss: 0.3518 - acc: 0.8525 - val_loss: 0.3519 - val_acc: 0.8492\n",
      "Epoch 35/200\n",
      "25000/25000 [==============================] - 352s 14ms/sample - loss: 0.3497 - acc: 0.8540 - val_loss: 0.3655 - val_acc: 0.8468\n",
      "Epoch 36/200\n",
      "25000/25000 [==============================] - 386s 15ms/sample - loss: 0.3455 - acc: 0.8584 - val_loss: 0.3505 - val_acc: 0.8509\n",
      "Epoch 37/200\n",
      "25000/25000 [==============================] - 470s 19ms/sample - loss: 0.3457 - acc: 0.8574 - val_loss: 0.3561 - val_acc: 0.8513\n",
      "Epoch 38/200\n",
      "25000/25000 [==============================] - 421s 17ms/sample - loss: 0.3383 - acc: 0.8594 - val_loss: 0.3423 - val_acc: 0.8552\n",
      "Epoch 39/200\n",
      "25000/25000 [==============================] - 472s 19ms/sample - loss: 0.3344 - acc: 0.8626 - val_loss: 0.3499 - val_acc: 0.8510\n",
      "Epoch 40/200\n",
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.3330 - acc: 0.8640 - val_loss: 0.3479 - val_acc: 0.8520\n",
      "Epoch 41/200\n",
      "25000/25000 [==============================] - 391s 16ms/sample - loss: 0.3282 - acc: 0.8660 - val_loss: 0.3368 - val_acc: 0.8565\n",
      "Epoch 42/200\n",
      "25000/25000 [==============================] - 401s 16ms/sample - loss: 0.3277 - acc: 0.8664 - val_loss: 0.3473 - val_acc: 0.8540\n",
      "Epoch 43/200\n",
      "25000/25000 [==============================] - 482s 19ms/sample - loss: 0.3220 - acc: 0.8677 - val_loss: 0.3398 - val_acc: 0.8572\n",
      "Epoch 44/200\n",
      "25000/25000 [==============================] - 412s 16ms/sample - loss: 0.3175 - acc: 0.8692 - val_loss: 0.3388 - val_acc: 0.8510\n",
      "Epoch 45/200\n",
      "25000/25000 [==============================] - 427s 17ms/sample - loss: 0.3174 - acc: 0.8715 - val_loss: 0.3317 - val_acc: 0.8560\n",
      "Epoch 46/200\n",
      "25000/25000 [==============================] - 382s 15ms/sample - loss: 0.3155 - acc: 0.8709 - val_loss: 0.3303 - val_acc: 0.8560\n",
      "Epoch 47/200\n",
      "25000/25000 [==============================] - 361s 14ms/sample - loss: 0.3113 - acc: 0.8738 - val_loss: 0.3259 - val_acc: 0.8603\n",
      "Epoch 48/200\n",
      "25000/25000 [==============================] - 509s 20ms/sample - loss: 0.3113 - acc: 0.8768 - val_loss: 0.3263 - val_acc: 0.8612\n",
      "Epoch 49/200\n",
      "25000/25000 [==============================] - 504s 20ms/sample - loss: 0.3022 - acc: 0.8786 - val_loss: 0.3282 - val_acc: 0.8571\n",
      "Epoch 50/200\n",
      "25000/25000 [==============================] - 532s 21ms/sample - loss: 0.3018 - acc: 0.8786 - val_loss: 0.3229 - val_acc: 0.8612\n",
      "Epoch 51/200\n",
      "25000/25000 [==============================] - 495s 20ms/sample - loss: 0.2995 - acc: 0.8792 - val_loss: 0.3220 - val_acc: 0.8624\n",
      "Epoch 52/200\n",
      "25000/25000 [==============================] - 518s 21ms/sample - loss: 0.2943 - acc: 0.8814 - val_loss: 0.3361 - val_acc: 0.8555\n",
      "Epoch 53/200\n",
      "25000/25000 [==============================] - 477s 19ms/sample - loss: 0.2961 - acc: 0.8804 - val_loss: 0.3221 - val_acc: 0.8559\n",
      "Epoch 54/200\n",
      "25000/25000 [==============================] - 525s 21ms/sample - loss: 0.2889 - acc: 0.8849 - val_loss: 0.3180 - val_acc: 0.8634\n",
      "Epoch 55/200\n",
      "25000/25000 [==============================] - 442s 18ms/sample - loss: 0.2923 - acc: 0.8827 - val_loss: 0.3216 - val_acc: 0.8587\n",
      "Epoch 56/200\n",
      "25000/25000 [==============================] - 377s 15ms/sample - loss: 0.2827 - acc: 0.8887 - val_loss: 0.3235 - val_acc: 0.8604\n",
      "Epoch 57/200\n",
      "25000/25000 [==============================] - 386s 15ms/sample - loss: 0.2863 - acc: 0.8837 - val_loss: 0.3300 - val_acc: 0.8539\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 362s 14ms/sample - loss: 0.2831 - acc: 0.8868 - val_loss: 0.3209 - val_acc: 0.8615\n",
      "Epoch 59/200\n",
      "25000/25000 [==============================] - 368s 15ms/sample - loss: 0.2787 - acc: 0.8896 - val_loss: 0.3165 - val_acc: 0.8642\n",
      "Epoch 60/200\n",
      "25000/25000 [==============================] - 394s 16ms/sample - loss: 0.2764 - acc: 0.8915 - val_loss: 0.3224 - val_acc: 0.8598\n",
      "Epoch 61/200\n",
      "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.2820 - acc: 0.8887 - val_loss: 0.3329 - val_acc: 0.8524\n",
      "Epoch 62/200\n",
      "25000/25000 [==============================] - 368s 15ms/sample - loss: 0.2743 - acc: 0.8931 - val_loss: 0.3171 - val_acc: 0.8611\n",
      "Epoch 63/200\n",
      "25000/25000 [==============================] - 392s 16ms/sample - loss: 0.2722 - acc: 0.8921 - val_loss: 0.3393 - val_acc: 0.8446\n",
      "Epoch 64/200\n",
      "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.2694 - acc: 0.8938 - val_loss: 0.3116 - val_acc: 0.8624\n",
      "Epoch 65/200\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.8956"
     ]
    }
   ],
   "source": [
    "history = imdb_model.fit(\n",
    "    train_text, \n",
    "    train_y, \n",
    "    epochs = 200,  \n",
    "    shuffle=True,\n",
    "    validation_data=[test_text, test_y]\n",
    ")\n",
    "\n",
    "utils.plot_history(history, ['loss', 'val_loss'])\n",
    "\n",
    "utils.plot_history(history, ['acc', 'val_acc'])\n",
    "\n",
    "imdb_model.evaluate(train_text, train_y)\n",
    "imdb_model.evaluate(test_text, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks (aka hooks) are functions called every N epochs that help you monitor and log the training process. By default, they will be called every 1 epoch. We will be using two common callbacks here: `EarlyStopping` and `ModelCheckpoint`. The first is used to prevent overfitting and the second is used to keep track of the best models we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=20,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model, Hope for the Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = imdb_model.fit(\n",
    "    train_text, \n",
    "    train_y, \n",
    "    epochs = 200,  \n",
    "    shuffle=True,\n",
    "    validation_data=[test_text, test_y]\n",
    ")\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Load the best model and do evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "imdb_model.load_weights(CPK_PATH)\n",
    "\n",
    "# Accuracy on validation \n",
    "imdb_model.evaluate(test_text, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
