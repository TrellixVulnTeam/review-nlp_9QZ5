{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Sentiment Analysis IMDB\n",
    "\n",
    "This notebook is a simple straight-forward way to achieve 90% accuracy on IMDB dataset. Note that this is not the only way to achieve such accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_proj_utils as utils\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth', 500)  # Set display column width to show more content\n",
    "\n",
    "# Load dataset, download if necessary\n",
    "train, test = utils.get_imdb_dataset()\n",
    "\n",
    "# Get a sample (head) of the data frame\n",
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "\n",
    "In this part,I will remove all the html label,punctuation and stopwords from the dataset. In order to reach a higher accuracy, I have selected 3000 most common word in the training data, and only the word in this list will be kept for further anylysis.\n",
    "1. Remove HTML tag (<br /> in this case) from the review text\n",
    "2. Remove punctuations (replace with whitespace)\n",
    "3. Split review text into tokens\n",
    "4. Remove tokens that are considered as \"stopwords\"\n",
    "5. For the rest, do lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "transtbl = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a text input and return the preprocessed string.\n",
    "def preprocessing(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Take a text input and return the preprocessed string.\n",
    "    i.e.: preprocessed tokens concatenated by whitespace\n",
    "    \"\"\"\n",
    "    line = line.replace('<br />','').translate(transtbl)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v')\n",
    "              for t in nltk.word_tokenize(line)\n",
    "              if t.lower() not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessing(\"I bought several books yesterday<br /> and I really love them!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "for df in train, test:\n",
    "    df['text_prep'] = df['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [w for text in tqdm_notebook(train['text_prep']) \n",
    "             for w in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FreqDist to get count for each word\n",
    "voca = nltk.FreqDist(all_words)\n",
    "print(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = [word for word, _ in voca.most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "import nlp_proj_utils as utils\n",
    "from tensorflow.keras.models import Model  \n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, word_to_vec_map = utils.load_glove_vecs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Select the first 200 words for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "print('max number of words in a sentence:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training/testing features into index list\n",
    "train_text = utils.sentences_to_indices(train['text_prep'], word_to_index, maxlen, topwords)\n",
    "test_text = utils.sentences_to_indices(test['text_prep'], word_to_index, maxlen, topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert label to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "test_y = test['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return a Keras Embedding Layer given word_to_vec mapping and word_to_index mapping\n",
    "    \n",
    "    Args:\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "\n",
    "    Return:\n",
    "        Keras.layers.Embedding: Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keras requires vocab length start from index 1\n",
    "    vocab_len = len(word_to_index) + 1  \n",
    "    emb_dim = list(word_to_vec_map.values())[0].shape[0]\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    return Embedding(\n",
    "        input_dim=vocab_len,\n",
    "        output_dim=emb_dim,\n",
    "        trainable=False,  # Indicating this is a pre-trained embedding \n",
    "        weights=[emb_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a LSTM Model\n",
    "\n",
    "I will use a two layer LSTM Model to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return the Keras model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: The dim of input layer\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Keras.models.Model: 2-layer LSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    sentence_indices = Input(shape=(input_dim,), dtype='int32')\n",
    "    \n",
    "    # Build embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_index, word_to_vec_map)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # 2-layer LSTM\n",
    "    X = LSTM(128, return_sequences=True, recurrent_dropout=0.5)(embeddings)  # N->N RNN，得到所有的a\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = LSTM(128, recurrent_dropout=0.5)(X)  # N -> 1 RNN\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model = build_model(\n",
    "    maxlen, \n",
    "    word_to_index, \n",
    "    word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = imdb_model.fit(\n",
    "    train_text, \n",
    "    train_y, \n",
    "    epochs = 200,  \n",
    "    shuffle=True,\n",
    "    validation_data=[test_text, test_y]\n",
    ")\n",
    "\n",
    "utils.plot_history(history, ['loss', 'val_loss'])\n",
    "\n",
    "utils.plot_history(history, ['acc', 'val_acc'])\n",
    "\n",
    "imdb_model.evaluate(train_text, train_y)\n",
    "imdb_model.evaluate(test_text, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks (aka hooks) are functions called every N epochs that help you monitor and log the training process. By default, they will be called every 1 epoch. We will be using two common callbacks here: `EarlyStopping` and `ModelCheckpoint`. The first is used to prevent overfitting and the second is used to keep track of the best models we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=20,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = imdb_model.fit(\n",
    "    train_text, \n",
    "    train_y, \n",
    "    epochs = 200,  \n",
    "    shuffle=True,\n",
    "    validation_data=[test_text, test_y]\n",
    ")\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Load the best model and do evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "imdb_model.load_weights(CPK_PATH)\n",
    "\n",
    "# Accuracy on validation \n",
    "imdb_model.evaluate(test_text, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = 'resources/model'\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "\n",
    "# Save model structure as json\n",
    "with open(os.path.join(model_root, \"model.json\"), \"w\") as fp:\n",
    "    fp.write(imdb_model.to_json())\n",
    "\n",
    "# Save model weights\n",
    "imdb_model.save_weights(os.path.join(model_root, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_index.pkl', 'wb') as fp:\n",
    "    pickle.dump(word_to_index, fp)\n",
    "    \n",
    "with open('word_to_vec_map.pkl', 'wb') as fp:\n",
    "    pickle.dump(word_to_vec_map, fp)\n",
    "    \n",
    "with open('topwords.pkl', 'wb') as fp:\n",
    "    pickle.dump(topwords, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
